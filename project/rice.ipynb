{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rice yield prediction project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a part of field based experiment conducted from 2018-2022 at International Rice Research Institute, Philipinnes where we want to compare how well multi-spectral data taken from drone remote sensing platform can help predict the complex trait(yield) compared to classical model i.e GBLUP(Genomic best linear unbiased predictor) which uses genomic data and this research is a chapter for master's thesis work.<br>\n",
    " In plant breeding the accuracy i.e correlation between the predicted value of germplasm and its actual value(response\"yield\")  is very important to assess the top performing germplasm. Based on estimated breeding value of germplasm they are selected in a breeding program which later is released as a variety commercially to grower and farmers. <br>\n",
    "Unlike other field, accuracy tend to be controlled by several factor in plant breeding because the accuracy of model to predict the trait is affected highly by the heritability of trait and since the complex trait such as yield, biomass has very low heritability, even the best model tend to have low accuracy and it cannot be more than the value of heritability of the trait. (Heritability is the proportion of phenotypic variance among individuals in a population that is due to heritable genetic effects which gets transferred to the progeny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashmita\\AppData\\Local\\Temp\\ipykernel_12644\\2443873611.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading 2022 phenotypic data\n",
    "phenoComb_2022_5b= pd.read_csv('phenoComb_2022_5b.csv')\n",
    "\n",
    "#Remove 'GERMPLASMNAME' and PLOTNO column from dataframe \n",
    "phenoComb_2022_5b = phenoComb_2022_5b.drop(['GERMPLASMNAME', 'PLOTNO'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE scores for each fold: [1.07650417 1.21235855 0.7709177  1.07681812 1.07364322 1.1198846\n",
      " 1.15785374 0.85336752 1.48149431 1.50269497]\n",
      "Average MSE: 1.1325536883214282\n",
      "Correlation coefficient: 0.427143679765113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "#splitting train and test and using 10 fold cv further to validate model\n",
    "X = phenoComb_2022_5b.drop(columns=['YLDTON'], axis=1) #yield measured in ton is response\n",
    "y = phenoComb_2022_5b['YLDTON']  \n",
    "imputer = SimpleImputer(strategy='mean') # imputing NA in response\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "# Initialize the RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor()\n",
    "scores = cross_val_score(rf_regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"MSE scores for each fold:\", -scores)\n",
    "print(\"Average MSE:\", -scores.mean())\n",
    "#obtaining accuracy here\n",
    "y_pred = cross_val_predict(rf_regressor, X, y, cv=10)\n",
    "correlation = np.corrcoef(y, y_pred)[0, 1]\n",
    "print(\"Correlation coefficient:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "Best parameters: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best score (negative MSE): -1.1171116551599414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],      \n",
    "    'max_depth': [10, 20, 30],      \n",
    "    'min_samples_split': [2, 5, 10],     \n",
    "    'min_samples_leaf': [1, 2, 4]      \n",
    "}\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fitting the grid search to the 2022_5b cohort data\n",
    "grid_search.fit(X, y_imputed)\n",
    "\n",
    "# Print the best parameters and the score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score (negative MSE):\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.13955917323931682\n",
      "Pearson Correlation Coefficient: 0.9751337904367676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Using the best parameters that were found in chunk above  to create a new RandomForestRegressor\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the entire training dataset\n",
    "best_rf.fit(X, y_imputed)\n",
    "\n",
    "# Evaluateingthe model on the test data that was splitted\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = best_rf.predict(X_test) \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)\n",
    "correlation, _ = pearsonr(y_test, y_pred)\n",
    "print(\"Pearson Correlation Coefficient:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performing feature importance just to check which feature are important and this feature importance from scikit uses the MDI method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = best_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns.tolist()  \n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(f\"{f + 1}. feature {feature_names[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "importances = best_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns.tolist() \n",
    "# Selecting the top 60 features\n",
    "top_n = 60\n",
    "top_indices = indices[:top_n]\n",
    "# Plot the feature importances of the top 60 features\n",
    "plt.figure(figsize=(10, 8)) \n",
    "plt.title(\"Top 60 Feature Importances\")\n",
    "bars = plt.bar(range(top_n), importances[top_indices], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=90)\n",
    "plt.xlim([-1, top_n])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout() \n",
    "for bar, imp in zip(bars, importances[top_indices]):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(imp, 4), ha='center', va='bottom', fontsize=8, rotation=90, color='black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting decision tree method on 2022 yield prediction data to compare the performance of random forest and gradient boosting decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "#splitting train and test\n",
    "X = phenoComb_2022_5b.drop(columns=['YLDTON'], axis=1) \n",
    "y = phenoComb_2022_5b['YLDTON']  \n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 243 candidates, totalling 2430 fits\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best negative MSE: -1.1465556741095224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],      \n",
    "    'learning_rate': [0.05, 0.1, 0.2],  \n",
    "    'max_depth': [3, 4, 5],              \n",
    "    'min_samples_split': [2, 5, 10],       \n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initializing the GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Initializing the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best negative MSE:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.011186986359468411\n",
      "Pearson Correlation Coefficient: 0.996651208270527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Using the best parameters to create a new gradientboostregressor using best parameter from above chunk of code\n",
    "best_gbd = GradientBoostingRegressor(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the entire training dataset\n",
    "best_gbd.fit(X, y_imputed)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE:\", mse)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation, _ = pearsonr(y_test, y_pred)\n",
    "print(\"Pearson Correlation Coefficient:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sets of model were fiting with 10 fold cv to assess the model performance and further metrices such as MSE and accuracy(relevant to plant breeding) was used to make the conclusion. Best parameters were serched by using gridsearch method which helped to fine tune the model further in both cases.<br>\n",
    "The MSE from random forest model and gradient boosting decision tree model is 0.13 and 0.011 respectively. The accuracy of random forest model is 0.97 & the GBDT is 0.99. So, the performance of gradient boosting deciison tree is better than random forest in one year of data using only spectral data.<br>\n",
    "However, the conclusion can only be made after fitting model for all year 2019-2022 and using genomic information in the model in full scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-499v-599v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
