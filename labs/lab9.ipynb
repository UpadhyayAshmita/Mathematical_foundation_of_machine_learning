{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26877cc-5d8b-44f8-b621-de305b9652aa",
   "metadata": {},
   "source": [
    "# 9. Keras and deep learning\n",
    "In this lab, we will learn how to use Keras to build deep learning models. We will use Keras to build a LSTM model for sentiment classification and a CNN model for digit recognition.\n",
    "You need to put in the code to complete the models in the blocks marked with `## YOUR CODE HERE` and `## END OF YOUR CODE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883c76f-1649-4fca-a014-d2b9a43520d0",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before you can start using Keras, you'll need to install TensorFolw, which includes Keras as part of its core library.\n",
    "```bash\n",
    "source activate {your_env}\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79e78b-b5cc-4aa8-a61e-12ad2fa2e3cf",
   "metadata": {},
   "source": [
    "## Basics of Keras\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "### 1. Initialize a model\n",
    "Start by creating a Sequential model and adding layers to it.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialize a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# this is equivalent to the above\n",
    "#model = Sequential([\n",
    "#    Dense(64, activation='relu', input_dim=100),\n",
    "#    Dense(10, activation='softmax')\n",
    "#])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "241c2cdc-b3eb-45b7-a9db-dceeb103d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b149c-8a46-4ff8-ac2e-89816cb5a8a4",
   "metadata": {},
   "source": [
    "### 2. Compile the model\n",
    "Compile the model with the appropriate loss function and optimizer.\n",
    "```python\n",
    "model.compile(loss='categorical_crossentropy', # loss function, binary_crossentropy for binary classification\n",
    "              optimizer='sgd', # stochastic gradient descent\n",
    "              metrics=['accuracy'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e4a5a28-4bff-470b-8297-fcef27b12070",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # loss function, binary_crossentropy for binary classification\n",
    "              optimizer='sgd', # stochastic gradient descent\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cc11c-b81d-472d-b1e4-4674d72f9643",
   "metadata": {},
   "source": [
    "### 3. Train the model\n",
    "Train the model with the training data.\n",
    "```python\n",
    "x_train = np.random.random((1000, 100))\n",
    "y_train = np.random.randint(2, size=(1000, 10))\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a06cd8d2-f6b2-4bfd-a075-5cb30e0292fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 12.8787 - accuracy: 0.0540\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 83.9980 - accuracy: 0.1150\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4780.5928 - accuracy: 0.1060\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 274764.1875 - accuracy: 0.1190\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 15929994.0000 - accuracy: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d1c020d250>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.random.random((1000, 100))\n",
    "y_train = np.random.randint(2, size=(1000, 10))\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d193a409-8a8e-41d1-83bb-cb470c81edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 107ms/step - loss: 118515872.0000 - accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "x_test = np.random.random((100, 100))\n",
    "y_test = np.random.randint(2, size=(100, 10))\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085601a-ad0f-4e35-b204-a4e2cc72c9b5",
   "metadata": {},
   "source": [
    "## Keras LSTM for IMDB sentiment classification\n",
    "The IMDB dataset is in `datasets/` of this repository. Use the following code the load the dataset and write a LSTM model to classify the sentiment of the reviews.\n",
    "```python\n",
    "import pandas as pd    # to load dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords   # to get a collection of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "data = pd.read_csv('../datasets/IMDB.csv')\n",
    "\n",
    "custom_path = '../datasets/IMDB.csv'\n",
    "\n",
    "# Append your custom path to the NLTK data path\n",
    "nltk.data.path.append(custom_path)\n",
    "\n",
    "nltk.download('stopwords', download_dir=custom_path)\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "x_data = data['review']       # Reviews/Input\n",
    "y_data = data['sentiment']    # Sentiment/Output\n",
    "# PRE-PROCESS REVIEW\n",
    "x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73f9a242-5c30-438d-a80d-2caf50daf4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../datasets...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords   # to get a collection of stopwords\n",
    "\n",
    "data = pd.read_csv('../datasets/IMDB.csv')\n",
    "custom_path = '../datasets'\n",
    "nltk.download('stopwords', download_dir=custom_path)\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "x_data = data['review']       # Reviews/Input\n",
    "y_data = data['sentiment']    # Sentiment/Output\n",
    "# PRE-PROCESS REVIEW\n",
    "x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22367208-ae65-4d74-90ee-c2a989f9068e",
   "metadata": {},
   "source": [
    "The tokenization of the reviews is done by the following code:\n",
    "```python\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=10000)    # num_words is the number of words to keep based on word frequency\n",
    "tokenizer.fit_on_texts(x_data)            # fit tokenizer to our training text data\n",
    "\n",
    "# retrieve the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "x_data = tokenizer.texts_to_sequences(x_data)  # convert our text data to sequence of numbers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe7c6a9-5878-4da9-b510-d9272206ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tokenization of the reviews is done by the following code:\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=10000)    # num_words is the number of words to keep based on word frequency\n",
    "tokenizer.fit_on_texts(x_data)            # fit tokenizer to our training text data\n",
    "\n",
    "# retrieve the word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "x_data = tokenizer.texts_to_sequences(x_data)  # convert our text data to sequence of numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0299a-454b-41f2-89e8-12a82e969c45",
   "metadata": {},
   "source": [
    "Now, complete the following code to create a LSTM model for the IMDB sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "226d5fa0-1a69-4115-8965-7651243ab909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 5s 16ms/step - loss: 0.4383 - accuracy: 0.7949 - val_loss: 0.3207 - val_accuracy: 0.8690\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2503 - accuracy: 0.9046 - val_loss: 0.3114 - val_accuracy: 0.8700\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.1619 - accuracy: 0.9420 - val_loss: 0.3352 - val_accuracy: 0.8631\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 0.0789 - accuracy: 0.9769 - val_loss: 0.4379 - val_accuracy: 0.8572\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0334 - accuracy: 0.9922 - val_loss: 0.4910 - val_accuracy: 0.8554\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.5259 - val_accuracy: 0.8345\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.5708 - val_accuracy: 0.8396\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.6297 - val_accuracy: 0.8583\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.6745 - val_accuracy: 0.8325\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0124 - accuracy: 0.9968 - val_loss: 0.8283 - val_accuracy: 0.8474\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.8283 - accuracy: 0.8471\n",
      "Test Accuracy: 0.847100019454956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, Dense, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_length = 100  # Define sequence length\n",
    "x_data = pad_sequences(x_data, maxlen=max_length)\n",
    "\n",
    "# Convert sentiments to binary labels\n",
    "y_data = np.where(y_data == 'positive', 1, 0)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=max_length),\n",
    "    SimpleRNN(units=32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce51eed-0eea-4f50-bfe9-110745830fd5",
   "metadata": {},
   "source": [
    "## Keras CNN for Digit Recognition\n",
    "In lab 5, we use the digit dataset. Now, we will use the same dataset to train a CNN model to recognize the digits.\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('../datasets/digits/Digits_X_train.csv').values\n",
    "y_train = pd.read_csv('../datasets/digits/Digits_y_train.csv').values\n",
    "X_test  = pd.read_csv('../datasets/digits/Digits_X_test.csv').values\n",
    "y_test  = pd.read_csv('../datasets/digits/Digits_y_test.csv').values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5789ec0b-d08a-41d3-84de-09a807d162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras CNN for Digit Recognition\n",
    "#In lab 5, we use the digit dataset. Now, we will use the same dataset to train a CNN model to recognize the digits.\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('../datasets/digits/Digits_X_train.csv').values\n",
    "y_train = pd.read_csv('../datasets/digits/Digits_y_train.csv').values.ravel()\n",
    "X_test  = pd.read_csv('../datasets/digits/Digits_X_test.csv').values\n",
    "y_test  = pd.read_csv('../datasets/digits/Digits_y_test.csv').values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e96d5-1024-46c7-83a2-fa752fbb35e1",
   "metadata": {},
   "source": [
    "Complete the following code to create a CNN model for the digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa56b310-a61b-464c-9aa0-9f608457a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 16)          160       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 4, 4, 16)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 4, 4, 32)          4640      \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38282 (149.54 KB)\n",
      "Trainable params: 38282 (149.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 1s 17ms/step - loss: 2.2934 - accuracy: 0.2321 - val_loss: 2.2792 - val_accuracy: 0.1815\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.2557 - accuracy: 0.2990 - val_loss: 2.2212 - val_accuracy: 0.3444\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2.1633 - accuracy: 0.5014 - val_loss: 2.0806 - val_accuracy: 0.7000\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.9473 - accuracy: 0.6778 - val_loss: 1.7818 - val_accuracy: 0.7407\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.5621 - accuracy: 0.7874 - val_loss: 1.3384 - val_accuracy: 0.7667\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.1015 - accuracy: 0.8180 - val_loss: 0.9449 - val_accuracy: 0.7852\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.7676 - accuracy: 0.8570 - val_loss: 0.7031 - val_accuracy: 0.8519\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5670 - accuracy: 0.8793 - val_loss: 0.5791 - val_accuracy: 0.8778\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.8867 - val_loss: 0.5032 - val_accuracy: 0.8926\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3985 - accuracy: 0.8932 - val_loss: 0.4318 - val_accuracy: 0.8889\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4087 - accuracy: 0.8711\n",
      "Accuracy:  0.8711110949516296\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, Flatten, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Reshape the data to 8 * 8 * 1\n",
    "X_train = X_train.reshape(X_train.shape[0], 8, 8, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 8, 8, 1)\n",
    "# Normalize the image data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(8, 8, 1), padding='same'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "## END OF YOUR CODE\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
